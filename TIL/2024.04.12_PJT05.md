# PJT
## 웹 크롤링 실습
### 파이썬으로 웹 페이지에 있는 정보 가져오는 방법
1. 누군가 업로드 해둔 데이터 다운로드 받기 (ex. 캐글)
2. 누군가 만들어 둔 API server를 활용하여 정보를 받아오기
3. 사람이 검색하는 것처럼 파이썬이 자동으로 검색 후 결과를 수집하는 방법
    - 이러한 기술을 크롤링(Crawling) 이라고 함
    - 이번 프로젝트에서 사용할 기술

### Quiz
- 데이터 사이언스에서는 먼저 데이터를 수집하는 것이 중요함
- Kaggle 같은 데이터 공유 플랫폼에서 수집된 데이터들을 쉽게 다운로드 받을 수 있지만 우리는 ---라는 기술을 사용하려 직접 데이터를 수집하고자 함
- 이 기술은?

### 프로젝트 목표
1. Django 없이 크롤링하는 방법 학습
2. 구글 검색 수를 크롤링하여 어떤 키워드가 더 많이 검색되는지 조사하기

## 웹 크롤링 이해하기
### 데이터 사이언스 프로세스
- 필요한 정보를 추출하는 5가지 단계
1. 문제 정의: 해결하고자 하는 문제 정의
2. 데이터 수집: 문제 해결에 필요한 데이터 수집
3. 데이터 전처리(정제): 실질적인 분석을 수행하기 위해 데이터를 가공하는 단계
    - 수집한 데이터의 오류 제거(결측치, 이상치), 데이터 형식 변환 등
4. 데이터 분석: 전처리가 완료된 데이터에서 필요한 정보를 추출하는 단계
5. 결과 해석 및 공유: 의사 결정에 활용하기 위해 결과를 해석하고 시각화 후 공유하는 단계

### 데이터 수집
- 웹 스크래핑(Web Scraping): 웹 페이지에서 데이터를 추출하는 기술
- 웹 크롤링(Web Crawling): 웹 페이지를 자동으로 탐색하고 데이터를 수집하는 기술
- Open API 활용: 공개된 API를 통해 데이터를 수집
- 데이터 공유 플랫폼 활용: 다양한 사용자가 데이터를 공유하고 활용할 수 있는 온라인 플랫폼
    - 종류: 캐글(Kaggle), Data.world, 데이콘(Dacon), 공공데이터포털 등

### 웹 크롤링
- 여러 웹 페이지를 돌아다니며 원하는 정보를 모으는 기술
- 원하는 정보를 추출하는 스크래핑(Scraping)과 여러 웹 페이지를 자동으로 탐색하는 크롤링(Crawling)의 개념을 합쳐 웹 크롤링이라고 부르ㅁ
- 즉, 웹 사이트들을 돌아다니며 필요한 데이터를 추출하여 활용할 수 있도록 자동화된 프로세스

### 웹 크롤링 프로세스
1. 웹 페이지 다운로드
    - 해당 웹 페이지의 HTML, CSS, JavaScript등의 코드를 가져오는 단계
2. 페이지 파싱
    - 다운로드 받은 코드를 분석하고 필요한 데이터를 추출하는 단계
3. 링크 추출 및 다른 페이지 탐색
    - 다른 링크를 추출하고, 다음 단계로 이동하여 원하는 데이터를 추출하는 단계
4. 데이터 추출 및 저장
    - 분석 및 시각화에 사용하기 위해 데이터를 처리하고 저장하는 단계

## 웹 크롤링 실습
### 준비 단계
- 실습 및 도전 과제에는 구글 검색 결과 페이지를 크롤링함
- 아래 필수 라이브러리를 설치 후 진행
    - `requests`: HTTP 요청을 보내고 응답을 받을 수 있는 모듈
    - `BeautifulSoup`: HTML 문서에서 원하는 데이터를 추출하는 데 사용되는 파이썬 라이브러리
    - `Selenium`: 웹 애플리케이션을 테스트하고 자동화하기 위한 파이썬 라이브러리
        - 웹 페이지의 동적인 컨텐츠를 가져오기 위해 사용함
- `$ pip install requests beautifulsoup4 selenium`

### `BeautifulSoup4` 요소 선택 메서드 종류
- `find()`: 태그를 사용하여 요소를 검색. 첫번째로 일치하는 요소를 반환
- `find_all()`: 태그를 사용하여 요소를 검색. 모든 일치하는 요소를 리스트로 반환
- `select()`: CSS 선택자를 사용하여 요소를 검색. 모든 일치하는 요소를 리스트로 반환
- `select_one()`: CSS 선택자를 사용하여 요소를 검색. 첫번째로 일치하는 요소를 반환
- `find_parent() / find_next_sibling() / find_previous_sibling()`: 태그를 사용하여 요소를 검색. 각각 일치하는 요소의 부모 / 다음 형제 요소 / 이전 형제 요소를 반환

## 도전과제
### Ver1
- 프로젝트 명: 키워드 검색량 분석을 위한 데이터 수집
- 목표
    - 크롤링을 통한 데이터 수집
    - 수집한 데이터를 DB에 저장하고, 저장한 데이터 활용하기
- 특징
    - 데이터 사이언스 패키지 사용
    - 수집한 데이터를 저장하고 활용할 수 있도록 DB 설계

### Ver2
- 프로젝트 명: DB를 활용한 웹 페이지 구현
- 목표
    - 영화, 회원, 댓글 간의 모델 관계가 형성된 애플리케이션 완성
- 특징
    - 1:N 관계 이해 및 응용