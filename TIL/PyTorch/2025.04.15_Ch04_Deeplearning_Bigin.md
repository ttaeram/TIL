# Chapter 04: 딥러닝 시작
## 01. 인공 신경망의 한계와 딥러닝 출현
- 오늘날 인공 신경망에서 이용하는 구조(입력층, 출력층, 가중치로 구성된 구조)는 프랭크 로젠블라트(Frank Rosenblatt)가 1957년에 고안한 퍼셉트론이라는 선형 분류기이다.
- 이 퍼셉트론은 오늘날 신경망(딥러닝)의 기원이 되는 알고리즘이다.
- 퍼셉트론은 다수의 신호(흐름이 있는)를 입력으로 받아 하나의 신호를 출력하는데, 이 신호를 입력으로 받아 ‘흐른다/안 흐른다(1 또는 0)’는 정보를 앞으로 전달하는 원리로 작동한다.

![](./assets/Ch04/Algorithm01.jpg)

- 그림과 같이 입력이 두 개(x1, x2) 있다고 할 때 컴퓨터가 논리적으로 인식하는 방식을 알아보기 위해 논리 게이트로 확인해 본다.

### AND 게이트
- AND 게이트는 모든 입력이 `1`일 때 작동한다.
- 입력 중 어떤 하나라도 `0`을 갖는다면 작동을 멈춘다.
- 진리표는 아래와 같다.

|x1|x2|y|
|:---:|:---:|:---:|
|0|0|0|
|1|0|0|
|0|1|0|
|1|1|1|

- AND 게이트에서 아래 그림과 같은 데이터 분류(검은색 점과 흰색 점)로 표현할 수 있다.

![](./assets/Ch04/Logic01.jpg)

### OR 게이트
- OR 게이트는 입력에서 둘 중 하나만 `1`이거나 둘 다 `1`일 때 작동한다.
- 입력 모두가 `0`을 갖는 경우를 제외한 나머지가 모두 `1` 값을 갖는다.
- 진리표는 아래와 같다.

|x1|x2|y|
|:---:|:---:|:---:|
|0|0|0|
|1|0|1|
|0|1|1|
|1|1|1|

- OR 게이트에서 아래 그림과 같은 데이터 분류(검은색 점과 흰색 점)로 표현할 수 있다.

![](./assets/Ch04/Logic02.jpg)

### XOR 게이트
- XOR 게이트는 배타적 논리합이라는 용어로 입력 두 개 중 한 개만 `1`일 때 작동하는 논리 연산이다.
- 진리표는 아래와 같다.

|x1|x2|y|
|:---:|:---:|:---:|
|0|0|0|
|1|0|1|
|0|1|1|
|1|1|0|

- XOR 게이트는 데이터가 비선형적으로 분리되기 때문에 제대로 된 분류가 어렵다.
- 단층 퍼셉트론에서는 AND, OR 연산에 대해서는 학습이 가능하지만, XOR에 대해서는 학습이 불가능하다.

![](./assets/Ch04/Logic03.jpg)

- 이를 극복하는 방안으로 입력층과 출력층 사이에 하나 이상의 중간층(은닉층)을 두어 비선형적으로 분리되는 데이터에 대해서도 학습이 가능하도록 다층 퍼셉트론(multi-layer perceptron)을 고안했다.
- 이때 입력층과 출력층 사이에 은닉층이 여러 개 있는 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 하며, 심층 신경망을 다른 이름으로 딥러닝이라고 한다.

## 02. 딥러닝 구조
### 2-1. 딥러닝 용어
- 딥러닝은 아래 그림과 같이 입력층, 출력층과 두 개 이상의 은닉층으로 구성되어 있다.
- 입력 신호를 전달하기 위해 다양한 함수도 사용하고 있는데, 신경망을 이루는 구성 요소에 대해 알아보자.

![](./assets/Ch04/Deeplearning01.jpg)

|구분|구성 요소|설명|
|:---:|:---:|:---:|
|층|입력층 (input layer)|데이터를 받아들이는 층|
|"|은닉층 (hidden layer)|모든 입력 노드부터 입력 값을 받아 가중합을 계산하고, 이 값을 활성화 함수에 적용하여 출력층에 전달하는 층|
|"|출력층 (output layer)|신경망의 최종 결괏값이 포함된 층|
|가중치 (weight)|=|노드와 노드 간 연결 강도|
|바이어스 (bias)|=|가중합에 더해 주는 상수로, 하나의 뉴런에서 활성화 함수를 거쳐 최종적으로 출력되는 값을 조절하는 역할을 함|
|가중합 (weighted sum), 전달 함수|=|가중치와 신호의 곱을 합한 것|
|함수|활성화 함수 (activation function)|신호를 입력받아 이를 적절히 처리하여 출력해주는 함수|
|"|손실 함수 (loss function)|가중치 학습을 위해 출력 함수의 결과와 실제 값 간의 오차를 측정하는 함수|

#### 가중치
- 가중치: 입력 값이 연산 결과에 미치는 영향력을 조절하는 요소
- 예를 들어 다음 그림에서 w1 값이 0 혹은 0과 가까운 0.001이라면, x1이 아무리 큰 값이라도 x1×w1 값은 0이거나 0에 가까운 값이 된다.
- 이와 같이 입력 값의 연산 결과를 조정하는 역할을 하는 것이 가중치이다.

![](./assets/Ch04/Deeplearning02.jpg)

#### 가중합 또는 전달 함수
- 가중합은 전달 함수라고도 한다.
- 가중합: 각 노드에서 들어오는 신호에 가중치를 곱해서 다음 노드로 전달되는데, 이 값들을 모두 더한 합계
- 노드의 가중합이 계산되면 이 가중합을 활성화 함수로 보내기 때문에 전달 함수(transfer function)라고도 한다.

![](./assets/Ch04/Deeplearning03.jpg)

- 가중합 구하는 공식:

![](./assets/Ch04/Formula01.jpg)

#### 활성화 함수
- 활성화 함수: 전달 함수에서 전달받은 값을 출력할 때 일정 기준에 따라 출력 값을 변화시키는 비선형 함수
- 활성화 함수 종류: 시그모이드(sigmoid), 하이퍼볼릭 탄젠트(hyperbolic tangent), 렐루(ReLU) 함수 등

#### (1) 시그모이드 함수
- 시그모이드 함수는 선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형해 준다.
- 주로 로지스틱 회귀와 같은 분류 문제를 확률적으로 표현하는 데 사용된다.
- 딥러닝 모델의 깊이가 깊어지면 기울기가 사라지는 ‘기울기 소멸 문제(vanishing gradient problem)’가 발생하여 딥러닝 모델에서는 잘 사용하지 않는다.
- 시그모이드 수식:

![](./assets/Ch04/Formula02.jpg)

![](./assets/Ch04/Graph01.jpg)

#### (2) 하이퍼볼릭 탄젠트 함수
- 하이퍼볼릭 탄젠트 함수는 선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변형해 준다.
- 시그모이드에서 결괏값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했지만, 기울기 소멸 문제는 여전히 발생한다.

![](./assets/Ch04/Graph02.jpg)

#### (3) 렐루 함수
- 렐루(ReLU) 함수는 입력(x)이 음수일 때는 0을 출력하고, 양수일 때는 x를 출력한다.
- 경사 하강법(gradient descent)에 영향을 주지 않아 학습 속도가 빠르고, 기울기 소멸 문제가 발생하지 않는 장점이 있다.
- 렐루 함수는 일반적으로 은닉층에서 사용되며, 하이퍼볼릭 탄젠트 함수 대비 학습 속도가 6배 빠르다.
- 음수 값을 입력받으면 항상 0을 출력하기 때문에 학습 능력이 감소하는데, 이를 해결하려고 리키 렐루(Leaky ReLU) 함수 등을 사용한다.

![](./assets/Ch04/Graph03.jpg)

#### (4) 리키 렐루 함수
- 리키 렐루(Leaky ReLU) 함수는 입력 값이 음수이면 0이 아닌 0.001처럼 매우 작은 수를 반환한다.
- 이렇게 하면 입력 값이 수렴하는 구간이 제거되어 렐루 함수를 사용할 때 생기는 문제를 해결할 수 있다.

![](./assets/Ch04/Graph04.jpg)

#### (5) 소프트맥스 함수
- 소프트맥스(softmax) 함수는 입력 값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 항상 1이 되도록 한다.
- 소프트맥스 함수는 보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용된다.
- 수식으로 표현하면 아래와 같다.

![](./assets/Ch04/Formula03.jpg)

- exp(x)(앞의 식에서는 exp(ak)와 exp(ai)를 의미)는 지수 함수(exponential function)이다.
- n은 출력층의 뉴런 개수, yk는 그중 k번째 출력을 의미한다.
- 이 수식처럼 소프트맥스 함수의 분자는 입력 신호 ak의 지수 함수, 분모는 모든 입력 신호의 지수 함수 합으로 구성된다.

- 아래는 렐루 함수와 소프트맥스 함수를 파이토치로 구현한 코드이다.
```py
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()

        # 은닉층
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.relu = torch.nn.ReLu(inplace=True)

        # 출력층
        self.out = torch.nn.Linear(n_hidden, n_output)
        self.softmax = torch.nn.Softmax(dim=n_output)

    def forward(self, x):
        x = self.hidden(x)

        # 은닉층을 위한 렐루 활성화 함수
        x = self.relu(x)
        x = self.out(x)

        # 출력층을 위한 소프트맥스 활성화 함수
        x = self.softmax(x)
        return x
```

#### 손실 함수
- 경사 하강법: 학습률(learning rate)과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트하는 방법
- 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동시키는 방법이라고 할 수 있다.
- 이때 오차를 구하는 방법이 손실 함수다.
- 손실 함수는 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표라고 할 수 있다.
- 이 값이 클수록 많이 틀렸다는 의미이고, 이 값이 ‘0’에 가까우면 완벽하게 추정할 수 있다는 의미이다.
- 대표적인 손실 함수: 평균 제곱 오차(Mean Squared Error, MSE)와 크로스 엔트로피 오차(Cross Entropy Error, CEE)

#### (1) 평균 제곱 오차
- 평균 제곱 오차(MSE): 실제 값과 예측 값의 차이(error)를 제곱하여 평균을 낸 것
- 실제 값과 예측 값의 차이가 클수록 평균 제곱 오차의 값도 커진다는 것은 반대로 생각하면 이 값이 작을수록 예측력이 좋다는 것을 의미한다.
- 평균 제곱 오차는 회귀에서 손실 함수로 주로 사용된다.
- 아래는 평균 제곱 오차를 구하는 수식이다.

![](./assets/Ch04/Formula04.jpg)

- 파이토치에서의 사용법:
```py
import torch

loss_fn = torch.nn.MSELoss(reduction='sum')
y_pred = model(x)
loss = loss_fn(y_pred, y)
```

#### (2) 크로스 엔트로피 오차
- 크로스 엔트로피 오차(CEE): 분류(classification) 문제에서 원-핫 인코딩(one-hot encoding)했을 때만 사용할 수 있는 오차 계산법
- 일반적으로 분류 문제에서는 데이터의 출력을 0과 1로 구분하기 위해 시그모이드 함수를 사용하는데, 시그모이드 함수에 포함된 자연 상수 e 때문에 평균 제곱 오차를 적용하면 매끄럽지 못한 그래프(울퉁불퉁한 그래프)가 출력된다.
- 따라서 크로스 엔트로피 손실 함수를 사용하는데, 이 손실 함수를 적용할 경우 경사 하강법 과정에서 학습이 지역 최소점에서 멈출 수 있다.
- 이것을 방지하고자 자연 상수 e에 반대되는 자연 로그를 모델의 출력 값에 취한다.
- 아래는 크로스 엔트로피를 구하는 수식이다.

![](./assets/Ch04/Formula05.jpg)

- 파이토치에서의 사용법:
```py
loss = nn.CrossEntropyLoss()

# torch.randn은 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용하여 숫자를 생성
input = torch.randn(5, 6, requires_grad=True)

# torch.empty는 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환
target = torch.empty(3, dtype=torch.long).random_(5)
output = loss(input, target)
output.backward()
```

### 2-2. 딥러닝 학습
- 딥러닝 학습은 크게 순전파와 역전파라는 두 단계로 진행된다.

![](./assets/Ch04/Deeplearning04.jpg)

- 첫 번째 단계인 순전파(feedforward)는 네트워크에 훈련 데이터가 들어올 때 발생하며, 데이터를 기반으로 예측 값을 계산하기 위해 전체 신경망을 교차해 지나간다.
- 즉, 모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 변환(가중합 및 활성화 함수)을 적용하여 다음 층(은닉층)의 뉴런으로 전송하는 방식이다.
- 네트워크를 통해 입력 데이터를 전달하며, 데이터가 모든 층을 통과하고 모든 뉴런이 계산을 완료하면 그 예측 값은 최종 층(출력층)에 도달하게 된다.
- 그다음 손실 함수로 네트워크의 예측 값과 실제 값의 차이(손실, 오차)를 추정한다.
- 이때 손실 함수 비용은 ‘0’이 이상적아다.
- 따라서 손실 함수 비용이 0에 가깝도록 하기 위해 모델이 훈련을 반복하면서 가중치를 조정한다.
- 손실(오차)이 계산되면 그 정보는 역으로 전파(출력층 → 은닉층 → 입력층)되기 때문에 역전파(backpropagation)라고 한다.
- 출력층에서 시작된 손실 비용은 은닉층의 모든 뉴런으로 전파되지만, 은닉층의 뉴런은 각 뉴런이 원래 출력에 기여한 상대적 기여도에 따라 값이 달라진다.
- 수학적으로 표현하면 예측 값과 실제 값 차이를 각 뉴런의 가중치로 미분한 후 기존 가중치 값에서 뺀다.
- 이 과정을 출력층 → 은닉층 → 입력층 순서로 모든 뉴런에 대해 진행하여 계산된 각 뉴런 결과를 또다시 순전파의 가중치 값으로 사용한다.

### 2-3. 딥러닝의 문제점과 해결 방안
- 딥러닝의 핵심은 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것이다.
- 그림과 같이 활성화 함수가 적용된 은닉층 개수가 많을수록 데이터 분류가 잘되고 있음을 볼 수 있다.

![](./assets/Ch04/Deeplearning05.jpg)

- 은닉층이 많으면 세가지 문제가 생긴다.

#### (1) 과적합 문제 발생
- 과적합(over-fitting)은 훈련 데이터를 과하게 학습해서 발생한다.
- 일반적으로 훈련 데이터는 실제 데이터의 일부분이라서 훈련 데이터를 과하게 학습했기 때문에 예측 값과 실제 값 차이인 오차가 감소하지만, 검증 데이터에 대해서는 오차가 증가할 수 있다.
- 이러한 관점에서 과적합은 훈련 데이터에 대해 과하게 학습하여 실제 데이터에 대한 오차가 증가하는 현상을 의미한다.

![](./assets/Ch04/Deeplearning06.jpg)

- 과적합을 해결하는 방법으로 드롭아웃(dropout)이 있다.
- 신경망 모델이 과적합되는 것을 피하기 위한 방법으로, 학습 과정 중 임의로 일부 노드들을 학습에서 제외시킨다.

![](./assets/Ch04/Deeplearning07.jpg)

- 아래는 파이토치에서 드롭아웃을 구현하는 코드
```py
class DropoutModel(torch.nn.Module):
    def __init__(self):
        super(DropoutModel, self).__init__()
        self.layer1 = torch.nn.Linear(784, 1200)

        # 50%의 노드를 무작위로 선택하여 사용하지 않겠다는 의미
        self.dropout1 = torch.nn.Dropout(0.5)
        self.layer2 = torch.nn.Linear(1200, 1200)
        self.dropout2 = torch.nn.Dropout(0.5)
        self.layer3 = torch.nn.Linear(1200, 10)

    def forward(self, x):
        x = F.relu(self.layer1(x))
        x = self.dropout1(x)
        x = F.relu(self.layer2(x))
        x = self.dropout2(x)
        return self.layer3(x)
```

#### (2) 기울기 소멸 문제 발생
- 기울기 소멸 문제는 은닉층이 많은 신경망에서 주로 발생하는데, 출력층에서 은닉층으로 전달되는 오차가 크게 줄어들어 학습이 되지 않는 현상이다.
- 즉, 기울기가 소멸되기 때문에 학습되는 양이 ‘0’에 가까워져 학습이 더디게 진행되다 오차를 더 줄이지 못하고 그 상태로 수렴하는 현상이다.
- 기울기 소멸 문제는 시그모이드나 하이퍼볼릭 탄젠트 대신 렐루 활성화 함수를 사용하면 해결할 수 있다.

![](./assets/Ch04/Deeplearning08.jpg)

#### (3) 성능이 나빠지는 문제 발생
- 경사 하강법은 손실 함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 계속 이동시키는 과정을 반복하는데, 이때 성능이 나빠지는 문제가 발생한다.

![](./assets/Ch04/Graph05.jpg)

- 이러한 문제점을 개선하고자 확률적 경사 하강법과 미니 배치 경사 하강법을 사용한다.
- 경사 하강법을 좀 더 알아보자.

![](./assets/Ch04/Deeplearning09.jpg)
