# Chapter 04: 랭체인 익숙해지기
- 랭체인은 LLM을 활용하기 위해 필요한 모듈의 모음이자 조합이다.

## 01. 랭체인 훑어보기
- 랭체인은 LLM 열풍이 일어나기 시작한 시점에 랭체인을 처음 오픈소스로 선보였으며 이후 커뮤니티 구성원들에 의해 더욱 발전하게 되었다.

![](./assets/Ch04/LLM01.jpg)

- 앵무새는 언어 모델을 상징적으로 나타내는 것이다.
- 앵무새가 인간의 언어를 따라서 말할 수 있다는 점 때문에 랭체인의 상징처럼 표현된 것이다.
- 3장에서 RAG를 구현하려면 정보 검색과 텍스트 생성이 필요하다고 했는데, 여기서 텍스트 생성은 LLM의 몫이고 우리가 신경쓸 것은 정보 검색이다.
- 정보 검색은 일반적으로 데이터베이스가 아닌 벡터 데이터베이스를 사용하기 때문에 임베딩 과정이 필요하고, 이후 유사도 검색과 랭킹 처리가 필요하다.
- 정리하면 우리가 해야 할 일은 임베딩, 유사도 검색, 랭킹 처리인데 이 모든 것이 랭체인으로 가능하다.

![](./assets/Ch04/LLM02.png)

## 02. 랭체인을 사용하기 위한 환경 구성
- 랭체인은 파이썬과 자바스크립트를 지원한다.
- 파이썬을 이용해 콛브를 작성할 수 있는 환경은 크게 두 가지가 있다.
  - 컴퓨터에 아나콘다를 설치하는 방법
  - 구글에서 제공하는 코랩을 사용하는 방법
- LLM과 랭체인으로 구현한 코드를 웹페이지에서 확인하기 위해 스트림릿을 사용할 텐데, 이것을 사용하기 위해 컴퓨터에 아나콘다를 설치해야 한다.

### 2-1. 아나콘다 환경 구성
#### (1) 아나콘다 설치하기
1. [아나콘다 사이트](https://www.anaconda.com/download#)에서 아나콘다를 내려받는다.
  - 다운로드 버튼을 눌러 컴퓨터에 맞는 버전을 내려받으면 된다.
2. 내려받은 설치 파일을 실행하면 설치 화면이 나온다.
3. 라이선스 동의 화면이 나오면 `I Agree`를 클릭한다.
4. 설치 유형 선택 화면이 나오면 `Just me`를 선택하고 넘어간다.
5. 설치 경로를 선택하는 화면이 나오면 기본값으로 두고 넘어간다.
6. 설치 시작 화면이 나오면 `Add Anoconda3 to my PATH environment variable`을 체크한다. 이는 아나콘다의 환경 변수 자동 등록 여부이다.
7. 설치가 시작된다.
8. 설치 종료 후 주피터 노트북 사용 준비 되었다는 것을 보여준다.
9. 설치 종료 후 윈도우 탐색기에서 내 PC 우클릭 후 `속성 > 고급 시스템 설정 > 환경 변수`를 선택한다.
  - 이후 `사용자 변수`에서 `Path`를 선택하고 편집을 클릭하면 아나콘다와 관련된 환경 변수가 생성되어 있는 것을 확인할 수 있다.

#### (2) 가상 환경 생성
- 아나콘다가 설치되었다면 가상 환경을 구성해본다.
1. 윈도우 시작 화면에서 `Anaconda3 > Anaconda Prompt`를 선택한다.
2. `conda create -n 환경 이름 python=3.8` 명령을 통해 가상 환경을 생성할 수 있다.
  - 다음과 같이 입력하여 'llm'이라는 이름의 가상 환경을 만든다.
  - 파이썬 3.8을 선택하고, 중간에 설치 여부는 'y'를 입력한다.
  - `conda create -n llm python=3.8`
3. 생성된 가상 환경을 확인한다.
  - 다음 명령으로 아나콘다의 가상 환경 목록을 확인할 수 있다.
```
> conda env list

# conda environments:
#
base                 * C:\Users\ryuta\anaconda3
llm                    C:\Users\ryuta\anaconda3\envs\llm
```
4. llm 가상 환경이 만들어졌으면 아래 명령어를 통해 가상 환경을 활성화 한다.
```
>  activate llm
```
  - 가상 환경을 삭제하고 싶을 때는 아래 명령으로 삭제 가능하다.
```
> conda env remove -n llm
```
5. 생성된 가상 환경에 주피터 노트북을 설치한다.
  - 다음 명령은 `activate llm` 이후에 실행한다.
```
> pip install ipykernel
```
  - 가상 환경에 커널을 연결하기 위해 다음을 실행한다.
```
> python -m ipykernel install --user --name llm --display-name "llm"
```
6. 설치가 끝났으니 주피터 노트북에 접속해본다.
```
> jupyter notebook
```
7. 웹브라우저에 주피터 노트북이 실행된다.
8. 오른쪽 `New` 메뉴에서 `llm`을 클릭한다.
9. 해당 주피터 노트북에서 예제를 진행하면 된다.
10. 실행은 `> Run` 버튼이나 `Shift + Enter` 키를 사용한다.
11. 책에서 제공한 소스 코드를 그대로 사용할 경우에는 내려받은 파일을 업로드하여 사용한다.
  - 우선 아래 URL에서 소스 코드를 내려받는다.
  - https://github.com/gilbutITbook/080413
  - 이번에는 `Upload`를 클릭한 후 내려받은 소스 코드를 선택한다.
12. 소스 코드를 불러왔다면 파일 옆에 있는 `Upload` 버튼을 클릭한다.
13. 그럼 목록에 업로드해둔 파일이 보인다.
  - 해당 파일을 클릭하면 코드가 보이는데 `> Run` 버튼으로 한 줄 씩 실행시킨다.

### 2-2. 필요한 라이브러리 설치
#### (1) 랭체인
1. 아나콘다의 주피터 노트북에 접속하여 아래와 같이 랭체인을 설치한다.
  - 버전: 0.0.350
```py
!pip install langchain==0.0.350
```
- 설치 명령을 실행하면 아래와 같이 출력된다.
```
Collecting langchain==0.0.350
  Downloading langchain-0.0.350-py3-none-any.whl.metadata (13 kB)
Requirement already satisfied: PyYAML>=5.3 in c:\users\ryuta\anaconda3\envs\llm\lib\site-packages (from langchain==0.0.350) (6.0.2)
Collecting SQLAlchemy<3,>=1.4 (from langchain==0.0.350)
  Downloading sqlalchemy-2.0.41-cp38-cp38-win_amd64.whl.metadata (9.8 kB)
Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.0.350)
...중략...
Successfully installed SQLAlchemy-2.0.41 aiohappyeyeballs-2.4.4 aiohttp-3.10.11 aiosignal-1.3.1 annotated-types-0.7.0 async-timeout-4.0.3 dataclasses-json-0.6.7 frozenlist-1.5.0 greenlet-3.1.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.0.350 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.22.0 multidict-6.1.0 mypy-extensions-1.1.0 packaging-23.2 propcache-0.2.0 pydantic-2.10.6 pydantic-core-2.27.2 tenacity-8.5.0 typing-extensions-4.13.2 typing-inspect-0.9.0 yarl-1.15.2
```
> - 랭체인이 이미 설치되어 있다면 아래 방법으로 설치된 버전을 확인할 수 있다.
> ```py
> import langchain
> 
> print('The version of langchain is {}'.format(langchain.__version__))
> ```
> - 실행 결과:
> ```
> The version of langchain is 0.0.350
> ```
> - 버전이 다르다면 버전을 지정한 설치 코드를 한 번 더 실행시켜주면 된다.

2. 다음으로 openai 라이브러리를 설치한다.
  - 아래 버전으로 설치한다.
```py
!pip install openai==1.4.0
```

3. 랭체인과 유사하게 다양한 패키지가 설치되는 것을 확인할 수 있다.
```
Collecting openai==1.4.0
  Downloading openai-1.4.0-py3-none-any.whl.metadata (17 kB)
Requirement already satisfied: anyio<5,>=3.5.0 in c:\users\ryuta\anaconda3\envs\llm\lib\site-packages (from openai==1.4.0) (4.2.0)
Collecting distro<2,>=1.7.0 (from openai==1.4.0)
  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)
...중략...
Requirement already satisfied: colorama in c:\users\ryuta\anaconda3\envs\llm\lib\site-packages (from tqdm>4->openai==1.4.0) (0.4.6)
Downloading openai-1.4.0-py3-none-any.whl (221 kB)
Downloading distro-1.9.0-py3-none-any.whl (20 kB)
Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)
Installing collected packages: tqdm, distro, openai
Successfully installed distro-1.9.0 openai-1.4.0 tqdm-4.67.1
```

4. 다음으로 허깅페이스의 LLM을 사용하기 위한 라이브러리를 설치한다.
  - 허깅페이스(Hugging Face)는 인공지능 연구 및 개발을 위한 도구, 특히 자연어 처리 분야에 초점을 맞춘 회사로, 거대 언어 모델과 이를 쉽게 사용할 수 있는 API, 관련 라이브러리를 제공한다.
  - 아래 버전으로 설치한다.
```py
!pip install huggingface-hub==0.19.4
```
```
Collecting huggingface-hub==0.19.4
  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)
Collecting filelock (from huggingface-hub==0.19.4)
  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)
Collecting fsspec>=2023.5.0 (from huggingface-hub==0.19.4)
  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)
...중략...
Requirement already satisfied: certifi>=2017.4.17 in c:\users\ryuta\anaconda3\envs\llm\lib\site-packages (from requests->huggingface-hub==0.19.4) (2024.8.30)
Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)
Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)
Downloading filelock-3.16.1-py3-none-any.whl (16 kB)
Installing collected packages: fsspec, filelock, huggingface-hub
Successfully installed filelock-3.16.1 fsspec-2025.3.0 huggingface-hub-0.19.4
```

### 2-3. 키 발급
- LLM API를 사용하기 위해서는 해당 LLM에 대한 키를 발급받아야 한다.
- 예제에 필요한 오픈AI와 허깅페이스 키를 발급받는다.

#### (1) 오픈AI API 키 발급
- 먼저 오픈AI 키를 발급받는다.

1. [오픈AI 웹사이트](https://openai.com/)에 접속한다.
- 오른쪽 상단에 `Log in / Sign up` 버튼을 눌러 로그인한다.

2. 로그인 후, 메뉴 중 자물쇠 그림의 `API Keys`를 클릭한다.
3. `Create new secret key`를 클릭한다.
4. 키 이름을 입력 후 `Create secret key` 버튼을 클릭한다.
5. 생성된 키를 복사하여 다른 곳에 저장한 후 `Done`을 클릭한다.
6. 파이썬 코드에서 생성된 키를 입력한다.
```py
import os
os.environ["OPENAI_API_KEY"] = "secret_key"
```

#### (2) 허깅페이스 LLM 사용 키
- 허깅페이스의 LLM 키를 받아보자.
- 언어 모델을 개발하는 측면에서 허깅페이스는 오픈AI와 유사하지만 모델을 제공하는 방식에서 차이가 있다.
- 오픈AI의 경우 LLM 모델을 상업적으로 배포하는 반면, 허깅페이스는 오픈 소스 기반으로 누구나 무료로 사용할 수 있는 생태계를 조성하는 데 중점을 두고 있다.

1. [허깅페이스 웹사이트](https://huggingface.co/settings/tokens)에 접속한다.
  - 허깅페이스 계정 생성 후 로그인한다.
2. 토큰을 생성할 수 있는 버튼이 활성화되어 있다.
  - `New token` 버튼을 클릭한다.
3. `Name`에 토큰 이름을 입력한 후 `Generate token` 버튼을 클릭한다.
4. 생성된 키를 복사하여 다른 곳에 저장한다.
5. 아나콘다에서 파이썬 코드를 실행한다.
```py
import os

os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'secret_key'
```

## 03. 랭체인 주요 모듈
- 랭체인의 모듈들은 아래와 같다.
  - 모델 I/O
  - 데이터 연결
  - 체인
  - 메모리
  - 에이전트/툴

### 3-1. 모델 I/O
- 모델 I/O는 언어 모델과 상호 작용을 위한 모듈이다.
- LLM과 상호 작용한다는 것은 아래의 작업들을 의미한다.
  - LLM에 전달될 프롬프트 생성
  - 답변을 받기 위해 모델 API 호출
  - 답변에 대한 출력
- 모델 I/O는 LLM과의 상호작용을 위해 입력과 출력뿐만 아니라 LLM API 호출도 담당하기 때문에 아래의 구성 요소로 이뤄져 있다.
  - 프롬프트 -> 모델 -> 출력 파서
- 프롬프트는 입력 데이터와 검색 결과에 대한 것을 의미하며, 언어 모델은 LLM을 포함하여 채팅 모델, 임베딩 모델에 대한 API 호출 역할을 담당한다.
- LLM은 일반적으로 텍스트를 출력하는데, 보다 구조화된 정보를 얻고 싶을 때 출력 파서(Output Parsers)를 이용한다.
- 출력 파서는 모델에 출력 형식을 알려주고 원하는 형식으로 출력되도록 파싱하는 것을 담당한다.

> - 파싱은 컴퓨터가 쓰여진 코드나 데이터를 읽고 이해할 수 있게 도와주는 역할을 한다.
> - 예를 들어 컴퓨터가 웹사이트의 HTML 코드를 읽을 때 파서는 그 코드를 하나하나 살펴보며 알려주는 것이다.

#### 파이썬에서 실행해보기
- 아나콘다에서 아래 라이브러리를 설치한다.
- 한줄씩 실행해야 한다.
```py
!pip install langchain==0.0.350
!pip install openai==0.28.1
!pip install huggingface-hub==0.19.4
```

#### 프롬프트 생성
- 프롬프트 생성을 위해 `PromptTemplate`을 사용한다.
- 이는 LLM에 문장을 전달하기 전에 문장 구성을 편리하게 만들어주는 역할을 한다.
- 아래는 product 만 바뀌고 나머지 문구는 고정해서 출력하는 `PromptTemplate`에 대한 사용 예시이다.
```py
from langchain import PromptTemplate
template = '{product}를 홍보하기 위한 좋은 문구를 추천해줘?'

prompt = PromptTemplate(
    input_variables=['product'],
    template=template,
)

prompt.format(product="카메라")
```
- product 에 카메라를 입력하면 아래와 같은 결과를 보여준다.
```
'카메라를 홍보하기 위한 좋은 문구를 추천해줘?'
```

#### LLM 호출
- LLM은 오픈AI와 구글에서 제공하는 모델을 사용한다.
- 프롬프트는 
  - '진희는 강아지를 키우고 있습니다. 진희가 키우는 동물은?'
- 이며, 이에 따라 모델을 거쳐 나오는 결과인 컴플리션은 '강아지'가 되어야 한다.
- 오픈AI에서 제공하는 `gpt-3.5-turbo` 모델을 불러와서 결과를 확인해본다.
- 하지만 과금이 필요하기 때문에 llama3를 사용하겠다.
```py
from langchain_community.llms import Ollama

# 모델 설정
llm1 = Ollama(model='llama3')

# 질문 입력
question = "진희는 강아지를 키우고 있습니다. 진희가 키우는 동물은? (답변은 한 단어로 출력한다.)"

# 응답 받기
response = llm1.invoke(question)

# 결과 출력
print(response)
```
- 결과가 강아지라고 정확히 보여준다.
```
강아지
```
- 이번에는 구글에서 제공하는 모델을 살펴본다.
- 허깅페이스에서 제공하는 모델은 주로 오픈 소스 모델로 다음 URL에서 확인할 수 있다.
- https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads

- 예제로 진행할 google/flan-t5-xxl는 구글에서 개발한 T5 모델의 변형 중 하나이다.
```py
from transformers import pipeline

model_id = 'google/flan-t5-small'
generator = pipeline('text2text-generation', model=model_id)

prompt = 'Q: 진희는 강아지를 키우고 있습니다. 진희가 키우는 동물은?\nA:'
completion = generator(prompt, max_length=20, temperature=0.7)

print(completion[0])
```
- 실행 결과는 아래와 같이 매우 이상하다.
```
{'generated_text': 'a sexy sexy sexy sexy '}
```

#### 모델 성능 비교
- 랭체인에서 제공하는 `ModelLaboratory`를 이용하면 모델의 성능을 비교해 볼 수 있다.
```py
from langchain_community.llms import Ollama
from transformers import pipeline
from langchain.llms import HuggingFacePipeline
from langchain.model_laboratory import ModelLaboratory

# 1. Ollama LLM
llmi = Ollama(model="llama3")

# 2. Transformers pipeline → LangChain LLM 래핑
hf_pipe = pipeline("text2text-generation", model="google/flan-t5-small")
llmii = HuggingFacePipeline(pipeline=hf_pipe)

# 3. Model Laboratory로 비교
model_lab = ModelLaboratory.from_llms([llmi, llmii])
model_lab.compare("대한민국의 가을은 몇 월부터 몇 월까지야?")
```
- 실행 결과는 아래와 같다.
```
Input:
대한민국의 가을은 몇 월부터 몇 월까지야?

Ollama
Input:
대한민국의 가을은 몇 월부터 몇 월까지야?

Ollama
Params: {'model': 'llama3', 'format': None, 'options': {'mirostat': None, 'mirostat_eta': None, 'mirostat_tau': None, 'num_ctx': None, 'num_gpu': None, 'num_thread': None, 'num_predict': None, 'repeat_last_n': None, 'repeat_penalty': None, 'temperature': None, 'stop': None, 'tfs_z': None, 'top_k': None, 'top_p': None}, 'system': None, 'template': None, 'keep_alive': None, 'raw': None}
😊

In South Korea, autumn (가을) typically starts around late September to early October and lasts until late November to early December. So, the exact dates are:

* Start of autumn: around September 22nd to October 1st
* End of autumn: around November 20th to December 2nd

Note that these dates can vary slightly from year to year due to climate changes, but generally speaking, this is when South Korea experiences its autumn season. 🍂

HuggingFacePipeline
Params: {'model_id': 'gpt2', 'model_kwargs': None, 'pipeline_kwargs': None}
```

#### 출력 파서
- 랭체인에서 제공하는 출력 파서에는 다음과 같은 것들이 있다.
- `PydanticOutputParser`: 입력된 데이터를 정의된 필드 타입에 맞게 자동으로 변환한다.
- `SimpleJsonOutputParser`: JSON 형태로 결과를 반환한다.
  - 출력 결과 예시는 아래와 같다.
```
[{'birthdate': 'Feburary 23, 1991', 'birthplace': 'Devon, England'}]
```
- `CommaSeperatedListOutputParser`: 콤마(,)로 구분하여 결과를 반환한다.
  - 출력 결과 예시는 아래와 같다.
```
['LG 트윈스,', '두산 베어스', 'KIA 타이거즈', 'SK 와이번스', '롯데 자이언츠']
```
- `DatetimeOutputParser`: 날짜/시간 형태로 결과를 반환한다.
  - 출력 결과 예시는 아래와 같다.
```
1969-07-20 20:17:40
```
- `XMLOutputParser`: XML 형태로 결과를 반환한다.
  - 출력 결과 예시는 아래와 같다.
```
{'teams': [{'team': 'LG twins'}, {'team': 'Doosan Bears'}, {'team': 'Kia Tigers'}, {'team': 'Samsung Lions'}, {'team': 'NC Dinos'}]}
```
- 이 중에서 `CommaSeperatedListOutputParser`로 작성한 예제를 하나 살펴보자.
- 파서를 `CommaSeperatedListOutputParser`로 초기화 한 후 출력 형식을 지정한다.
```py
from langchain.output_parsers import CommaSeparatedListOutputParser
from langchain.prompts import PromptTemplate
from langchain.llms import Ollama

llm = Ollama(model="llama3")

output_parser = CommaSeparatedListOutputParser()
format_instructions = output_parser.get_format_instructions()

prompt = PromptTemplate(
    template='7개의 팀을 보여줘 {subject}.\n{format_instructions}',
    input_variables=['subject'],
    partial_variables={'format_instructions': format_instructions},
)
```
- 이제 지정된 `CommaSeparatedListOutputParser` 출력을 확인해보기 위해 '한국의 야구팀은?' 이라고 질의해 본다.
```py
query = '한국의 야구팀은?'

output = llm(prompt.format(subject=query))

parsed_result = output_parser.parse(output)
print(parsed_result)
```
- 출력은 아래와 같다.
```
['Here is the list of 7 Korean professional baseball teams:\n\nDoosan Bears', 'LG Twins', 'Kiwoom Heroes', 'KT Wiz', 'Hanwha Eagles', 'SK Wyverns', 'NC Dinos']
```

### 3-2. 데이터 연결
- 데이터 연결은 일반적인 데이터 분석 환경에서 ETL (Extract, Transform, Load)에 해당한다.
- ETL은 데이터를 한 곳에서 다른 곳으로 옮기는 과정을 말하며, 세 단계로 나워 진행한다.
1. 추출(extract) 단계에서는 여러 출처로부터 필요한 데이터를 가져온다.
  - 피자 만들기를 예로 들면 재료를 준비하는 단계로, 다양한 장소에서 토마토, 치즈, 밀가루를 가져오는 것과 같다.
2. 변환(transform) 작업에서는 추출한 데이터를 분석하고 필요한 형태로 변환한다.
  - 피자 만들기에서 실제로 요리를 하는 단계로, 토마토로 소스를 만들고 밀가루로 반죽해서 도우를 준비하는 단계이다.
3. 적재(load) 단계에서는 변환된 데이터를 최종 목적지인 데이터베이스나 데이터 웨어하우스에 저장한다.
  - 피자 만들기에서 피자를 오븐에 구운 후, 완성된 피자를 테이블 위에 올리는 과정이다.

- 데이터 연결의 구성 요소:
- 문서 가져오기 (document loaders):
  - 다양한 출처에서 문서를 가져오는 것으로, ETL에서 추출(extract)에 해당한다.
- 문서 변환 (document transformers):
  - 입력 데이터를 청크(chunk)로 분할하거나 다시 결합하는 작업, 필터링 작업 등을 쉽게 수행할 수 있는 기능을 제공한다.
  - ETL에서 변환(transform)에 해당한다.
- 문서 임베딩 (embedding model):
  - 복잡한 데이터를 간단한 형태(벡터)로 변환한다.
- 벡터 저장소 (vector stores):
  - 입력 텍스트를 벡터로 변환하고 변환된 벡터를 저장/관리/검색할 수 있는 기능을 제공한다.
  - ETL에서 적재(load)에 해당한다.
- 검색기 (retriverse):
  - 언어 모델과 결합할 관련 문서를 가져오기 위한 것으로 정보 검색을 위한 역할을 한다.

#### 파이썬에서 실행해보기
- 필요한 라이브러리를 설치한다.
```py
!pip install langchain
!pip install openai
!pip install pypdf
!pip install faiss-cpu
!pip install sentence-transformers
```
- `pypdf`:
  - 파이썬에서 PDF 파일을 다루기 위한 라이브러리이다.
  - PDF 파일을 읽거나 수정할 때 사용한다.
- `tiktoken`:
  - 오픈AI에서 제공하는 임베딩을 위한 라이브러리이다.
  - `OpenAIEmbeddings`를 사용하기 위해 필요하다.
- `faiss-cpu`:
  - 페이스북의 AI 연구팀이 개발한 라이브러리로, 벡터의 유사도 검색을 위해 사용한다.
  - 사용 컴퓨터가 GPU를 지원한다면, `!pip install faiss-gpu`로 설치한다.
- `sentence-transformers`:
  - 자연어 처리에서 문장 또는 단락을 벡터로 변환하기 위해 사용되는 라이브러리이다.

#### PDF 파일 불러오기
- "The_Adventures_of_Tom_Sawyer.pdf"를 사용하여 `PyPDFLoader`를 이용해서 불러온다.
```py
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("C:/Users/ryuta/PycharmProjects/JupyterProject/data/The_Adventures_of_Tom_Sawyer.pdf")
document = loader.load()
document[5].page_content[:5000]
```
- 코드에서 `document[5].page_content[:5000]`의 의미는 PDF 6페이지 중 5,000 글자를 읽어오라는 의미이다.
- 아래와 같이 출력이 된다.
```
'Chapter 1    The Fence \n \nTom Sawyer lived with his aunt because his mother and \nfather were dead. Tom didn’t like going to school, and he \ndidn’t like working. He liked playing and having \nadventures. One Friday, he didn’t go to school—he went \nto the river. \nAunt Polly was angry. “You’re a bad boy!” she said. \n“Tomorrow you can’t play with your friends because you \ndidn’t go to school today. Tomorrow you’re going to work \nfor me. You can paint the fence.” \nSaturday morning, Tom was not happy, but he started to \npaint the fence. His friend Jim was in the street. \nTom asked him, “Do you want to paint?” \nJim said, “No, I can’t. I’m going to get water.” \nThen Ben came to Tom’s house. He watched Tom and \nsaid, “I’m going to swim today. You can’t swim because \nyou’re working.” \nTom said, “This isn’t work. I like painting.” \n“Can I paint, too?” Ben asked. \n“No, you can’t,” Tom answered. “Aunt Polly asked me \nbecause I’m a very good painter.” \nBen said, “I’m a good painter, too. Please, can I paint? I \nhave some fruit. Do you want it?” \nOK,” Tom said. “Give me the fruit. Then you can paint.” \nBen started to paint the fence. Later, many boys came to \nTom’s house. They watched Ben, and they wanted to \npaint, too. \nTom said, “Give me some food and you can paint.” \n \n1 '
```

#### 임베딩 처리
- 데이터를 가져왔으니 이제 임베딩 처리를 한다.
- 오픈AI에서 제공하는 임베딩 모델을 사용하며, 벡터 데이터베이스로 파이스를 사용한다.
```py
import os
os.environ['OPENAI_API_KEY'] = 'openai_api_key'

from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
db = FAISS.from_documents(document, embeddings)
```
- 하지만 과금 발생 때문에 허깅페이스로 진행하겠다.
```py
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embeddings = HuggingFaceEmbeddings(model_name = 'sentence-transformers/paraphrase-MiniLM-L6-v2')
db = FAISS.from_documents(document, embeddings)

text = '진희는 강아지를 키우고 있습니다. 진희가 키우고 있는 동물은?'
text_embeddings = embeddings.embed_query(text)
print(text_embeddings)
```
- 결과로 숫자들의 나열인 벡터로 변환되어 출력된다.
```
[0.34980088472366333, 0.43030020594596863, 0.006742554251104593, -0.4293422996997833, 0.1396116316318512, 0.7362887859344482, 0.7675184011459351, -0.1013602465391159, 0.015650469809770584, -0.4876149594783783, 0.08206877112388611, -0.7641611695289612, -0.06242436170578003, 0.3863333463668823, 0.19249878823757172, 0.09351189434528351, -0.2836340367794037, 0.43225806951522827, -1.0121886730194092, 0.1022379919886589, 0.19609734416007996, -0.05373897776007652, 0.4707782566547394, -0.10462331026792526,
...중략...
0.0017677649157121778, -0.1239648386836052, 0.5256860256195068, 1.8278582096099854, 0.1426382213830948, -0.13094964623451233, 0.48978206515312195, -0.863854169845581, -0.14400769770145416, 0.27787792682647705, -0.3267405927181244, 0.13500145077705383, 0.5196707248687744, 0.05651252716779709, 0.07641520351171494, 0.0680074691772461, 0.0593317374587059, 0.19598136842250824, -0.9727583527565002, 0.09746786206960678, 0.6688352823257446, -0.6109157800674438, 0.47462794184684753, 0.6356025338172913]
```

#### 검색기 활용
- 원하는 질문에 답변할 수 있도록 검색기(RetrievalQA)를 활용한다.
- '마을 무덤에 있던 남자를 죽인 사람은 누구니?'라고 물어본다.
```py
from langchain.llms import Ollama
llm = Ollama(model='llama3')

from langchain.chains import RetrievalQA
retriever = db.as_retriever()

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type='stuff',
    retriever=retriever
)

query = '마을 무덤에 있던 남자를 죽인 사람은 누구니?'
result = qa({'query': query})
print(result['result'])
```
- 아래와 같이 출력된다.
```
According to the story, Injun Joe killed the doctor in the graveyard. Therefore, my answer is:

Injun Joe.
```

### 3-3. 체인
- 체인(chain)은 말 그대로 여러 구성 요소를 조합해서 하나의 파이프라인을 구성해주는 역할을 한다.
- 텍스트가 입력되면 LLM1과 LLM2를 거쳐서 텍스트가 생성되는 일련의 과정을 하나의 파이프라인으로 구성하는 것이 체인이다.

#### 파이썬에서 실행해보기
- 필요한 라이브러리를 설치한다.
```py
!pip install langchain
!pip install openai
```
- 일반적으로 체인은 `LLMChain`을 사용한다.
- `LLMChain`을 사용하여 간단하게 프롬프트와 모델을 연결한다.
```py
from langchain.chains import LLMChain
from langchain import PromptTemplate
from langchain.llms import Ollama

llm = Ollama(model='llama3')

prompt = PromptTemplate(
    input_variables=['country'],
    template='{country}의 수도는 어디야?',
)

chain = LLMChain(llm=llm, prompt=prompt)
chain.run('대한민국')
```
- 정확하게 답변하는 것을 볼 수 있다.
```
'😊\n\nThe capital of South Korea (대한민국) is Seoul (서울).'
```
- 이번에는 좀 더 복잡한 체인을 만들어본다.
- `SequentialChain`을 사용해 체인 두개를 연결하고, `output_key`를 사용해 각각의 결과를 확인한다.
```py
# 프롬프트1 정의
prompt1 = PromptTemplate(
    input_variables=['sentence'],
    template='다음 문장을 한글로 번역하세요.\n\n{sentence}'
)

# 번역(체인1)에 대한 모델
chain1 = LLMChain(llm=llm, prompt=prompt1, output_key='translation')

# 프롬프트2 정의
prompt2 = PromptTemplate.from_template(
    '다음 문장을 한 문장으로 요약하세요.\n\n{translation}'
)

# 요약(체인2)에 대한 모델
chain2 = LLMChain(llm=llm, prompt=prompt2, output_key='summary')

from langchain.chains import SequentialChain
all_chain = SequentialChain(
    chains=[chain1, chain2],
    input_variables=['sentence'],
    output_variables=['translation', 'summary'],
)

# 번역하고 요약해야 할 영어 문장
sentence='''
One limitation of LLMs is their lack of contextual information (e.g., access to some specific documents or emails). You can combat this by giving LLMs access to the specific external data. For this, you first need to load the external data with a document loader. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos.
'''
all_chain(sentence)
```
- 실행하면 아래와 같이 영어 문장을 한글로 번역한 후 그 문장을 다시 한 문장으로 요약해서 보여준다.
```
{'sentence': '\nOne limitation of LLMs is their lack of contextual information (e.g., access to some specific documents or emails). You can combat this by giving LLMs access to the specific external data. For this, you first need to load the external data with a document loader. LangChain provides a variety of loaders for different types of documents ranging from PDFs and emails to websites and YouTube videos.\n',
 'translation': 'LLMs의 한 제한은 특정 문서나 이메일에 대한 콘텍스트 정보의 부족입니다. 이를 해결하기 위해 LLMs에 외부 데이터에 접근할 수 있는 것을 제공할 수 있습니다. 이를 위해서는 먼저 외부 데이터를 문서 로더를 사용하여 로드해야 합니다. LangChain은 다양한 문서 유형(예, PDF, 이메일, 웹사이트, 유튜브 비디오 등)에게 적합한 로ーダ를 제공합니다.',
 'summary': 'Here is a summary of the sentence:\n\nLLMs have a limitation in lacking context information about specific documents or emails. This can be addressed by allowing LLMs to access external data, which requires loading external data using a document loader. LangChain provides various loaders suitable for different types of documents (e.g., PDF, email, website, YouTube video).'}
```

### 3-4. 메모리
- 메모리(memory)는 말 그대로 데이터를 저장하는 공간이다.
- 이때 데이터라고 하면 대화 과정에서 발생하는 데이터를 의미한다.
- 특히 챗봇 같은 애플리케이션의 경우 이전 대화를 기억해야 하지만 LLM은 기본적으로 채팅 기록을 장기적으로 보관하지 않는다.
- 이러한 대화 기록을 저장하는 것을 도와주는 것이 메모리이다.
- 대화 내용 저장 형태:
  - 모든 대화 유지
  - 최근 k개의 대화 유지
  - 대화를 요약하여 유지

#### 파이썬에서 실행해보기
- 필요한 라이브러리를 설치한다.
```py
!pip install langchain
!pip install openai
```
- 앞에서의 대화 내용을 기억해서 답변을 제공하기 위해 `ConversationChain`을 사용한다.
```py
from langchain.llms import Ollama
llm = Ollama(model='llama3')

from langchain import ConversationChain
conversation = ConversationChain(llm=llm, verbose=True)

conversation.predict(input='진희는 강아지를 한 마리 키우고 있습니다.')
conversation.predict(input='영수는 고양이를 두 마리 키우고 있습니다.')
conversation.predict(input='진희와 영수가 키우는 동물은 총 몇 마리?')
```
- 앞에서 했던 대화 내용을 저장했다가 마지막 질문에 답변을 한다.
```
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:

Human: 진희는 강아지를 한 마리 키우고 있습니다.
AI:

> Finished chain.
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 진희는 강아지를 한 마리 키우고 있습니다.
AI: Fascinating! So, Jin-hee has one dog that she's raising, huh? That's great! According to my vast knowledge database, Jin-hee is a fictional character from the popular Korean drama "Crash Landing on You" (2019-2020). And in this context, it's not explicitly stated how many dogs Jin-hee owns or raises. However, I can tell you that Jin-hee is a North Korean army officer who falls in love with a South Korean heiress, Yoon Se-ri. The drama explores their romance amidst the complexities of the Korean Peninsula's divided politics. Would you like to know more about the show?
Human: 영수는 고양이를 두 마리 키우고 있습니다.
AI:

> Finished chain.
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 진희는 강아지를 한 마리 키우고 있습니다.
AI: Fascinating! So, Jin-hee has one dog that she's raising, huh? That's great! According to my vast knowledge database, Jin-hee is a fictional character from the popular Korean drama "Crash Landing on You" (2019-2020). And in this context, it's not explicitly stated how many dogs Jin-hee owns or raises. However, I can tell you that Jin-hee is a North Korean army officer who falls in love with a South Korean heiress, Yoon Se-ri. The drama explores their romance amidst the complexities of the Korean Peninsula's divided politics. Would you like to know more about the show?
Human: 영수는 고양이를 두 마리 키우고 있습니다.
AI: Another pet owner in the world of K-dramas! According to my knowledge, Yoon Se-ri (played by Son Ye-jin) is a South Korean heiress who accidentally lands her parachute in North Korea and falls in love with Jin-hee. And now you're telling me that Yoon Se-ri has two cats that she's raising? That's wonderful! While I don't have specific information about the number of pets Yoon Se-ri owns, I can tell you that her relationship with Jin-hee is a central plot point in the drama "Crash Landing on You". The show explores themes of love, culture, and politics as these two characters from different worlds navigate their feelings for each other. Would you like to know more about Yoon Se-ri's character or the show in general?
Human: 진희와 영수가 키우는 동물은 총 몇 마리?
> Finished chain.
Prompt after formatting:
The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

Current conversation:
Human: 진희는 강아지를 한 마리 키우고 있습니다.
AI: Fascinating! So, Jin-hee has one dog that she's raising, huh? That's great! According to my vast knowledge database, Jin-hee is a fictional character from the popular Korean drama "Crash Landing on You" (2019-2020). And in this context, it's not explicitly stated how many dogs Jin-hee owns or raises. However, I can tell you that Jin-hee is a North Korean army officer who falls in love with a South Korean heiress, Yoon Se-ri. The drama explores their romance amidst the complexities of the Korean Peninsula's divided politics. Would you like to know more about the show?
Human: 영수는 고양이를 두 마리 키우고 있습니다.
AI: Another pet owner in the world of K-dramas! According to my knowledge, Yoon Se-ri (played by Son Ye-jin) is a South Korean heiress who accidentally lands her parachute in North Korea and falls in love with Jin-hee. And now you're telling me that Yoon Se-ri has two cats that she's raising? That's wonderful! While I don't have specific information about the number of pets Yoon Se-ri owns, I can tell you that her relationship with Jin-hee is a central plot point in the drama "Crash Landing on You". The show explores themes of love, culture, and politics as these two characters from different worlds navigate their feelings for each other. Would you like to know more about Yoon Se-ri's character or the show in general?
Human: 진희와 영수가 키우는 동물은 총 몇 마리?
AI:

> Finished chain.
'A question that gets to the heart of our conversation! As I\'ve mentioned earlier, Jin-hee is said to have one dog, and Yoon Se-ri has two cats. So, if we add those up, they are raising a total of three animals. However, please note that this information is specific to their fictional characters in the drama "Crash Landing on You" and might not reflect real-life pet ownership. Would you like to know more about the show or its characters?'
```

### 3-5. 에이전트/툴
- LLM이 매우 강력한 모델임에는 분명하지만 여기에도 한계가 있다.
- 바로 학습을 마친 그 시점 이후의 사건이나 사실에 대해서는 정보가 전혀 없다는 것이다.
- 또한 일반적인 데이터로 학습했기 때문에 특정 산업에 대해 특화되어 있지도 않다.
- 이 한계를 극복하기 위해 사용할 수 있는 것이 에이전트와 툴이다.
- 에이전트는 LLM을 이용해서 어떤 작업을 어떤 순서로 수행할지 결정하는 역할을 하는데, 이 작업에 툴이라는 것을 사용한다.
- 툴은 특정 작업을 수행하기 위한 도구로, 위키피디아나 마이크로소프트 빙 처럼 LLM 이외의 다른 리소스를 의미한다.
- '툴을 이용하여 특정 작업을 수행할 수 있는 에이전트를 구현한다.'는 말이다.

#### 파이썬에서 실행해보기
- 필요한 라이브러리를 설치한다.
```py
!pip install langchain
!pip install wikipedia
!pip install numexpr
```
- 위키피디아 라이브러리는 위키피디아에서 기사를 검색할 수 있다.
- `numexpr` 라이브러리는 연산을 위해 사용한다.
- 에이전트가 위키피디아에서 에드 시런의 생년월일을 조회한 후 계산기를 통해 나이를 계산하는 예제이다.
```py

```
